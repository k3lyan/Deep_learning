# Deep_learning
Deep learning from scratch: every deep learning engineer should have a good understanding on how deep learning works. One must understand it from the mathematical background. 

A good introduction to deep learning is Ian Goodfellow, Yoshua Bengio and Aaron Courville's book, that you can find here:
https://www.deeplearningbook.org/

As your general uderstanding grows, you must be interested as going deeper into some specific topics tackled by deep learning. Here are some fundamental research articles I suggest:

## Modern practical deep learning:
The Marginal Value of Adaptive Gradient Methods in Machine Learning
https://arxiv.org/abs/1705.08292

Asynchronous Stochastic Gradient Descent with Delay Compensation for Distributed Deep Learning
https://arxiv.org/abs/1609.08326

Asynchronous Stochastic Gradient Descent with Variance Reduction for Non-Convex Optimization
https://arxiv.org/abs/1604.03584

Large Scale Distributed Deep Networks
https://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf

Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
Sergey Ioffe, Christian Szegedy
https://arxiv.org/abs/1502.03167

Xavier (Glorot) Normal Initializer
http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf

He Normal Initializer
http://arxiv.org/abs/1502.01852

To understand Nesterov Momentum:
Advances in optimizing Recurrent Networks by Yoshua Bengio, Section 3.5
http://arxiv.org/pdf/1212.0901v2.pdf

## Model interpretation (What-Why-How):
“Why should I trust you?” Explaining the Predictions of Any Classifier
Ribeiro, Singh, Guestrin
https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf

## Bayesian vs frequentist:
TRUTH AND PROBABILITY (1926) - Ramsey
https://core.ac.uk/download/pdf/7048428.pdf

## Information Theory:
Elements of Information Theory - Cover and Thomas (2006)
http://www.inference.org.uk/itprnn/book.html

Information Theory, Inference, and Learning Algorithms - MacKay (2003)
http://www.inference.org.uk/itprnn/book.pdf

## Convolutional neural network:
Gradient-based Learning applied to document recognition
Yann LeCun, Léon Bottou, Yoshua Bengio and Patrick Haffner - 1998
http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf

Recepteive fields and functional architecture of monkey striate cortex
Hubel and Wiesel - 1968
https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1968.sp008455

A Neural Algorithm of Artistic Style
https://arxiv.org/abs/1508.06576

SSD: Single Shot MultiBox Detector
https://arxiv.org/abs/1512.02325
Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG)
https://arxiv.org/abs/1409.1556

Deep Residual Learning for Image Recognition
https://arxiv.org/abs/1512.03385

Going Deeper with Convolutions (Inception)
https://arxiv.org/abs/1409.4842


## Recurrent neural network:
Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Chung 2014
https://arxiv.org/pdf/1412.3555v1.pdf

Unsupervised / GAN:
Generative Visual Manipulation on the Natural Image Manifold
https://www.youtube.com/watch?v=9c4z6YsBGQ0

Generative Adversarial Nets (Goodfellow 2014)
https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
Radford, A., Metz, L., and Chintala, S. (2015). 
Unsupervised representation learning with deep convolutional generative adversarial networks
https://arxiv.org/abs/1511.06434v2

Striving for Simplicity: The All Convolutional Net (Springenberg et al., 2015)
https://arxiv.org/abs/1412.6806

## Unsupervised NLP:
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation
https://nlp.stanford.edu/pubs/glove.pdf

Neural Word Embedding as Implicit Matrix Factorization
http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf

Hierarchical Softmax
http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf

More about Hierarchical Softmax
http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf

Distributed Representations of Words and Phrases and their Compositionality
https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf

## Advanced NLP:
Bidirectional Recurrent Neural Networks
https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf

Translation Modeling with Bidirectional Recurrent Neural Networks
http://emnlp2014.org/papers/pdf/EMNLP2014003.pdf

Sequence to Sequence Learning with Neural Networks
https://arxiv.org/abs/1409.3215

A Neural Conversational Model
https://arxiv.org/abs/1506.05869v3
Neural Machine Translation by Jointly Learning to Align and Translate (Attention)
https://arxiv.org/abs/1409.0473

Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems (Simplified Attention)
https://arxiv.org/abs/1512.08756

Memory Networks
https://arxiv.org/abs/1410.3916

Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks
http://arxiv.org/abs/1502.05698

End-To-End Memory Networks
http://arxiv.org/abs/1503.08895

Ask Me Anything: Dynamic Memory Networks for Natural Language Processing
https://arxiv.org/abs/1506.07285

WaveNet
https://deepmind.com/blog/wavenet-generative-model-raw-audio/

Tacotron
https://google.github.io/tacotron/

Tacotron 2
https://research.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html

An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling
https://arxiv.org/abs/1803.01271
(just released March 2018!)

Relational recurrent neural networks
https://arxiv.org/abs/1806.01822
